<!-- Generated by SpecStory -->

# Debugging Flaky Test in Versioning (2025-04-14 23:16:19)

_**User**_

Can you help me debug a flaky functional test error in tests/versioning_3_test.go?

---

_**Assistant**_

I'll help you investigate the flaky test in `tests/versioning_3_test.go`. First, let me look at the test file to understand what we're dealing with.


---

Read file: tests/versioning_3_test.go

---

I see this is a test suite for testing versioning functionality in Temporal. The file contains many test cases that verify different aspects of versioning behavior. To help debug the flaky test, I'll need to know:

1. Which specific test is failing?
2. What is the error message or behavior you're seeing?
3. How often does it fail (e.g. 1 in 10 runs, 1 in 100 runs)?

The test file contains several test cases that could potentially be flaky, including:
- `TestPinnedTask_NoProperPoller`
- `TestUnpinnedTask_NonCurrentDeployment`
- `TestUnpinnedTask_OldDeployment`
- `TestWorkflowWithPinnedOverride_Sticky`/`NoSticky`
- `TestQueryWithPinnedOverride_Sticky`/`NoSticky`
- And many others

Could you please provide more details about which test is failing and what the failure looks like? This will help me focus on the relevant parts of the code and identify potential causes of the flakiness.

Some common causes of flaky tests in this type of test suite could be:
1. Race conditions in concurrent operations
2. Timing issues with task polling and completion
3. State synchronization between different components
4. Issues with sticky task queues
5. Problems with deployment version transitions

Once you provide more details about the specific test and failure, I can help investigate the root cause and suggest fixes.

---

_**User**_

This is the error:
versioning_3_test.go:1856: 
        	Error Trace:	/home/runner/work/temporal/temporal/tests/versioning_3_test.go:1856
        	            				/home/runner/work/temporal/temporal/tests/versioning_3_test.go:1863
        	            				/opt/hostedtoolcache/go/1.23.2/x64/src/runtime/asm_amd64.s:1700
        	Error:      	Received unexpected error:
        	            	failed to poll workflow task: taskpoller test helper timed out while waiting for the PollWorkflowTaskQueue API response, meaning no workflow task was ever created
        	Test:       	TestVersioning3FunctionalSuite/TestUnpinnedWorkflowWithRamp_ToUnversioned/NoTaskForwardNoPollForwardAllowSync

---

_**Assistant**_

Thank you for providing the error details. This is a timeout error occurring in the `TestUnpinnedWorkflowWithRamp_ToUnversioned` test, specifically in the `NoTaskForwardNoPollForwardAllowSync` configuration. The error indicates that the test helper timed out while waiting for a workflow task to be created.

Let me look at the relevant test code to understand what's happening.



---

Read file: tests/versioning_3_test.go

---

Let me also check the test helper code that's timing out:



---

Read file: tests/versioning_3_test.go

---

Based on the code and error message, I can identify several potential causes for this flaky test:

1. **Race Condition in Deployment Setup**: The test sets up two deployments (v1 and v2) with v1 as current and v2 ramping at 50%. The test waits for deployment data propagation and sets the current deployment, but there might be a race condition where the task queue isn't fully ready when the workflow starts.

2. **Task Queue Partitioning**: The test is using partitioned task queues (as seen in the `SetupSuite` where it sets `MatchingNumTaskqueueReadPartitions` and `MatchingNumTaskqueueWritePartitions` to 4). This could lead to tasks being routed to different partitions, potentially causing delays or missed tasks.

3. **Timeout Configuration**: The test helper is using a 10-second timeout for polling workflow tasks (`taskpoller.WithTimeout(10*time.Second)`), which might not be enough in some cases, especially with the task queue partitioning.

4. **Worker Configuration**: The test uses `numPollers` for `MaxConcurrentWorkflowTaskPollers`, but we don't see its value. If this is too low, it could cause delays in task processing.

Here are some suggestions to fix the flakiness:

1. **Increase Timeout**: Try increasing the timeout in the `pollWftAndHandle` function:
```go
resp, err := poller.PollWorkflowTask(
    &workflowservice.PollWorkflowTaskQueueRequest{
        DeploymentOptions: tv.WorkerDeploymentOptions(true),
        TaskQueue:         tq,
    },
).HandleTask(tv, handler, taskpoller.WithTimeout(30*time.Second)) // Increase from 10s to 30s
```

2. **Add Additional Verification**: Add a check to ensure the task queue is fully ready before starting the workflow:
```go
s.waitForDeploymentDataPropagation(tv1, false, tqTypeWf, tqTypeAct)
s.setCurrentDeployment(tv1)
// Add a small delay to ensure propagation is complete
time.Sleep(100 * time.Millisecond)
```

3. **Verify Task Queue State**: Add logging or verification of the task queue state before starting the workflow to ensure it's in the expected state.

4. **Increase Worker Pollers**: If `numPollers` is low, consider increasing it to ensure better task distribution.

5. **Add Retry Logic**: Add retry logic around the workflow execution to handle transient failures:
```go
var run sdkclient.WorkflowRun
var err error
for i := 0; i < 3; i++ {
    run, err = sdkClient.ExecuteWorkflow(ctx, sdkclient.StartWorkflowOptions{TaskQueue: tv1.TaskQueue().GetName()}, "wf")
    if err == nil {
        break
    }
    time.Sleep(100 * time.Millisecond)
}
s.NoError(err)
```

Would you like me to help implement any of these suggestions? Also, it would be helpful to know:
1. How often does this test fail?
2. Are there any patterns to when it fails (e.g., during CI, specific environments)?
3. Do you have any logs from the test failure that might show more context?

---

_**User**_

It fails in CI when the persistence layer is cassandra. I'm pretty sure waitForDeploymentDataPropagation already checks to see that the task queue has the expected version, so the sleep won't help

---

_**Assistant**_

Ah, that's a crucial detail - the test fails specifically with Cassandra persistence. This suggests the issue might be related to Cassandra's eventual consistency model and how it interacts with the versioning system.

Let me look at the `waitForDeploymentDataPropagation` implementation to understand how it verifies the task queue state:



---

Read file: tests/versioning_3_test.go

---

Let me also check the Cassandra-specific code to understand how it handles versioning data:




---

<details>
            <summary>Searched codebase "cassandra versioning deployment data persistence" â€¢ **25** results</summary>
        
| File | Lines |
|------|-------|
| `schema/cassandra/temporal/versioned/v1.6/cluster_metadata_info.cql` | L1-10 |
| `schema/cassandra/temporal/schema.cql` | L216-230 |
| `schema/cassandra/temporal/versioned/v1.8/task_queue_user_data.cql` | L1-18 |
| `schema/cassandra/temporal/versioned/v1.9/queues.cql` | L1-25 |
| `schema/cassandra/temporal/versioned/v1.10/nexus_incoming_services.cql` | L1-16 |
| `schema/cassandra/temporal/versioned/v1.11/nexus_endpoints.cql` | L1-18 |
| `schema/cassandra/temporal/schema.cql` | L102-148 |
| `common/persistence/cassandra/version_checker.go` | L42-87 |
| `common/persistence/cassandra/cluster_metadata_store.go` | L144-192 |
| `schema/cassandra/temporal/versioned/v1.0/schema.cql` | L98-159 |
| `schema/cassandra/temporal/versioned/v1.0/schema.cql` | L49-98 |
| `schema/cassandra/temporal/versioned/v1.1/cluster_metadata.cql` | L1-3 |
| `schema/cassandra/temporal/schema.cql` | L148-216 |
| `schema/cassandra/temporal/schema.cql` | L54-102 |
| `common/persistence/cassandra/nexus_endpoint_store.go` | L44-87 |
| `common/persistence/tests/cassandra_test_util.go` | L190-254 |
| `common/persistence/cassandra/nexus_endpoint_store.go` | L87-140 |
| `schema/cassandra/temporal/versioned/v1.6/manifest.json` | L1-10 |
| `schema/cassandra/temporal/versioned/v1.3/manifest.json` | L1-9 |
| `schema/cassandra/temporal/versioned/v1.8/manifest.json` | L1-7 |
| `schema/cassandra/temporal/versioned/v1.2/queue.cql` | L1-3 |
| `tools/cassandra/cqlclient.go` | L168-218 |
| `common/persistence/cassandra/matching_task_store.go` | L534-578 |
| `common/persistence/cassandra/queue_v2_store.go` | L38-65 |
| `common/persistence/cassandra/queue_v2_store.go` | L321-394 |

</details>

---

After analyzing the code and the Cassandra schema, I can see that the issue is likely related to how Cassandra handles eventual consistency with the task queue user data. The key points are:

1. The test is using partitioned task queues (4 partitions) as seen in the `SetupSuite`:
```go
dynamicconfig.MatchingNumTaskqueueReadPartitions.Key():  4,
dynamicconfig.MatchingNumTaskqueueWritePartitions.Key(): 4,
```

2. The task queue user data is stored in Cassandra with a schema that uses a single partition per namespace for task queue user data:
```sql
PRIMARY KEY ((namespace_id), build_id, task_queue_name)
```

3. The test is setting up a ramping deployment (50%) and running multiple workflows to test the distribution.

The flakiness is likely occurring because:
1. Cassandra's eventual consistency model means that updates to the task queue user data might not be immediately visible across all nodes
2. The test is using a single partition for task queue user data but multiple partitions for the task queues themselves
3. The `waitForDeploymentDataPropagation` function might not be waiting long enough for Cassandra's consistency to catch up

Here's what I suggest to fix this:

1. Increase the consistency level for task queue user data operations. This can be done by modifying the Cassandra configuration to use `QUORUM` or `ALL` consistency level for these operations.

2. Add a stronger verification in `waitForDeploymentDataPropagation` that checks not just the presence of the data but also its consistency across all partitions. The current implementation might be too optimistic about Cassandra's consistency guarantees.

3. Consider adding a small delay after setting the deployment data to allow Cassandra's consistency to catch up, even though you mentioned the sleep won't help. The issue might be that we need to wait for the consistency level to be satisfied, not just for the data to be written.

Would you like me to help implement any of these suggestions? I can also look deeper into the Cassandra configuration and task queue user data implementation if needed.

---

_**User**_

How could I do this: Add a stronger verification in waitForDeploymentDataPropagation that checks not just the presence of the data but also its consistency across all partitions. The current implementation might be too optimistic about Cassandra's consistency guarantees.


---

_**Assistant**_

Let me look at the current implementation of `waitForDeploymentDataPropagation` and then suggest how to strengthen it:




---

Read file: tests/versioning_3_test.go

---

Let me see the rest of the function:




---

Read file: tests/versioning_3_test.go

---

I see. The current implementation checks each partition individually and waits until all partitions have the expected deployment data. However, it doesn't verify that the data is consistent across partitions. Here's how we can strengthen it:

1. First, let's modify the function to collect the deployment data from all partitions and verify they match:

```go
func (s *Versioning3Suite) waitForDeploymentDataPropagation(
    tv *testvars.TestVars,
    unversionedRamp bool,
    tqTypes ...enumspb.TaskQueueType,
) {
    ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)
    defer cancel()

    v := s.GetTestCluster().Host().DcClient().GetValue(dynamicconfig.MatchingNumTaskqueueReadPartitions.Key())
    s.NotEmpty(v, "versioning tests require setting explicit number of partitions")
    count, ok := v[0].Value.(int)
    s.True(ok, "partition count is not an int")
    partitionCount := count

    type partAndType struct {
        part int
        tp   enumspb.TaskQueueType
    }
    remaining := make(map[partAndType]struct{})
    for i := 0; i < partitionCount; i++ {
        for _, tqt := range tqTypes {
            remaining[partAndType{i, tqt}] = struct{}{}
        }
    }

    f, err := tqid.NewTaskQueueFamily(s.NamespaceID().String(), tv.TaskQueue().GetName())
    s.NoError(err)

    // Map to store the deployment data for each partition
    partitionData := make(map[partAndType]*persistencespb.TaskQueueUserData)

    s.Eventually(func() bool {
        // First pass: collect data from all partitions
        for pt := range remaining {
            partition := f.TaskQueue(pt.tp).NormalPartition(pt.part)
            res, err := s.GetTestCluster().MatchingClient().GetTaskQueueUserData(
                ctx,
                &matchingservice.GetTaskQueueUserDataRequest{
                    NamespaceId:   s.NamespaceID().String(),
                    TaskQueue:     partition.RpcName(),
                    TaskQueueType: partition.TaskType(),
                })
            if err != nil {
                return false
            }
            partitionData[pt] = res.GetUserData().GetData()
        }

        // Second pass: verify consistency and check for expected data
        for pt, data := range partitionData {
            perTypes := data.GetPerType()
            if perTypes == nil {
                continue
            }

            // Verify this partition has the expected deployment
            hasExpectedDeployment := false
            deps := perTypes[int32(pt.tp)].GetDeploymentData().GetDeployments()
            for _, d := range deps {
                if d.GetDeployment().Equal(tv.Deployment()) {
                    hasExpectedDeployment = true
                    break
                }
            }

            if unversionedRamp {
                if perTypes[int32(pt.tp)].GetDeploymentData().GetUnversionedRampData() != nil {
                    hasExpectedDeployment = true
                }
            } else {
                versions := perTypes[int32(pt.tp)].GetDeploymentData().GetVersions()
                for _, d := range versions {
                    if d.GetVersion().Equal(tv.DeploymentVersion()) {
                        hasExpectedDeployment = true
                        break
                    }
                }
            }

            if hasExpectedDeployment {
                delete(remaining, pt)
            }
        }

        // If we still have remaining partitions, check if they're consistent with each other
        if len(remaining) > 0 {
            // Get a reference partition's data to compare against
            var referenceData *persistencespb.TaskQueueUserData
            for pt := range remaining {
                if referenceData == nil {
                    referenceData = partitionData[pt]
                    continue
                }
                // Compare this partition's data with the reference
                if !proto.Equal(referenceData, partitionData[pt]) {
                    // Data is inconsistent, keep waiting
                    return false
                }
            }
        }

        return len(remaining) == 0
    }, 10*time.Second, 100*time.Millisecond)
}
```

The key changes in this implementation are:

1. We now collect all partition data first before making any decisions
2. We verify that the data is consistent across partitions by comparing them using `proto.Equal`
3. We only consider a partition "ready" if it has both the expected deployment data AND is consistent with other partitions
4. We maintain the same timeout and interval as before, but now we're doing a more thorough check

This stronger verification should help catch cases where:
1. Some partitions have the data but others don't
2. Different partitions have different versions of the data
3. The data is inconsistent across partitions

The function will keep retrying until either:
1. All partitions have the expected deployment data and are consistent with each other, or
2. The timeout is reached

Would you like me to help implement this change? We can also add logging to help debug any consistency issues that might arise.

---

_**User**_

the partitions mentioned in waitForDeploymentDataPropagation are Task Queue Partitions, not Cassandra Partitions

---

_**Assistant**_

Ah, you're absolutely right - I apologize for the confusion. The partitions in `waitForDeploymentDataPropagation` are Task Queue Partitions, which are used for load balancing and scaling within the Temporal matching service, not Cassandra partitions.

Let me look at how the task queue user data is actually stored in Cassandra to understand the consistency issue better:




---

Read file: schema/cassandra/temporal/versioned/v1.8/task_queue_user_data.cql

---

Now I understand better. The task queue user data is stored in Cassandra with:
- A single partition per namespace (`namespace_id` is the partition key)
- The data for all task queues in a namespace is stored in that single partition
- The clustering key is `(build_id, task_queue_name)`

Given this, the flakiness is likely due to Cassandra's eventual consistency model when reading from this single partition. Even though we're checking multiple task queue partitions, they all read from the same Cassandra partition.

Here's how we can strengthen the verification:

```go
func (s *Versioning3Suite) waitForDeploymentDataPropagation(
    tv *testvars.TestVars,
    unversionedRamp bool,
    tqTypes ...enumspb.TaskQueueType,
) {
    ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)
    defer cancel()

    v := s.GetTestCluster().Host().DcClient().GetValue(dynamicconfig.MatchingNumTaskqueueReadPartitions.Key())
    s.NotEmpty(v, "versioning tests require setting explicit number of partitions")
    count, ok := v[0].Value.(int)
    s.True(ok, "partition count is not an int")
    partitionCount := count

    type partAndType struct {
        part int
        tp   enumspb.TaskQueueType
    }
    remaining := make(map[partAndType]struct{})
    for i := 0; i < partitionCount; i++ {
        for _, tqt := range tqTypes {
            remaining[partAndType{i, tqt}] = struct{}{}
        }
    }

    f, err := tqid.NewTaskQueueFamily(s.NamespaceID().String(), tv.TaskQueue().GetName())
    s.NoError(err)

    // Map to store the last seen version for each partition
    lastVersions := make(map[partAndType]int64)

    s.Eventually(func() bool {
        allConsistent := true
        for pt := range remaining {
            partition := f.TaskQueue(pt.tp).NormalPartition(pt.part)
            res, err := s.GetTestCluster().MatchingClient().GetTaskQueueUserData(
                ctx,
                &matchingservice.GetTaskQueueUserDataRequest{
                    NamespaceId:   s.NamespaceID().String(),
                    TaskQueue:     partition.RpcName(),
                    TaskQueueType: partition.TaskType(),
                })
            if err != nil {
                return false
            }

            // Check if we've seen this version before
            currentVersion := res.GetUserData().GetVersion()
            if lastVersion, exists := lastVersions[pt]; exists && lastVersion != currentVersion {
                // Version changed, reset consistency check
                allConsistent = false
            }
            lastVersions[pt] = currentVersion

            perTypes := res.GetUserData().GetData().GetPerType()
            if perTypes == nil {
                continue
            }

            hasExpectedDeployment := false
            deps := perTypes[int32(pt.tp)].GetDeploymentData().GetDeployments()
            for _, d := range deps {
                if d.GetDeployment().Equal(tv.Deployment()) {
                    hasExpectedDeployment = true
                    break
                }
            }

            if unversionedRamp {
                if perTypes[int32(pt.tp)].GetDeploymentData().GetUnversionedRampData() != nil {
                    hasExpectedDeployment = true
                }
            } else {
                versions := perTypes[int32(pt.tp)].GetDeploymentData().GetVersions()
                for _, d := range versions {
                    if d.GetVersion().Equal(tv.DeploymentVersion()) {
                        hasExpectedDeployment = true
                        break
                    }
                }
            }

            if hasExpectedDeployment {
                delete(remaining, pt)
            }
        }

        // Only return true if we have no remaining partitions AND all versions are consistent
        return len(remaining) == 0 && allConsistent
    }, 10*time.Second, 100*time.Millisecond)
}
```

The key changes in this version:

1. We track the version number of the task queue user data for each partition
2. We consider the data "consistent" only when:
   - All partitions have the expected deployment data
   - The version numbers have stabilized (not changing between checks)
3. We keep retrying until both conditions are met

This should help catch cases where:
1. The Cassandra partition is still being updated (versions changing)
2. Some task queue partitions have stale data
3. The data is not yet consistent across all task queue partitions

The function will keep retrying until either:
1. All task queue partitions have the expected deployment data and their versions have stabilized, or
2. The timeout is reached

Would you like me to help implement this change? We can also add logging to help debug any consistency issues that might arise.

---

_**User**_

Is it possible for me to label and save this conversation to come back to later, while we work on something else?

---

_**Assistant**_

I don't have a built-in way to label or save conversations, but you can:

1. Copy the conversation URL from your browser and save it somewhere
2. Take a screenshot of the conversation
3. Copy and paste the relevant parts into a document or note
4. Bookmark the conversation in your browser

Would you like me to summarize the key points we've discussed so far to make it easier to save?

---

_**User**_

sure, but can you be really brief

---

_**Assistant**_

Here's the brief summary:

**Issue**: Flaky test in `versioning_3_test.go` with Cassandra persistence, specifically in `TestUnpinnedWorkflowWithRamp_ToUnversioned`.

**Root Cause**: Cassandra's eventual consistency model affecting task queue user data reads from a single namespace partition.

**Proposed Fix**: Strengthen `waitForDeploymentDataPropagation` to verify data consistency by tracking version numbers across task queue partitions and ensuring they stabilize.

**Key Files**:
- `tests/versioning_3_test.go`
- `schema/cassandra/temporal/versioned/v1.8/task_queue_user_data.cql`

---

_**User**_

What line was the failure at again?

---

_**Assistant**_

The failure was at line 1856 in `versioning_3_test.go`:

```
versioning_3_test.go:1856: 
        Error Trace:	/home/runner/work/temporal/temporal/tests/versioning_3_test.go:1856
        Error:      	Received unexpected error:
                   	failed to poll workflow task: taskpoller test helper timed out while waiting for the PollWorkflowTaskQueue API response, meaning no workflow task was ever created
        Test:       	TestVersioning3FunctionalSuite/TestUnpinnedWorkflowWithRamp_ToUnversioned/NoTaskForwardNoPollForwardAllowSync
```

---

_**User**_

Are you super duper sure that Cassandra might not have the same task queue user data on all nodes?

---

_**Assistant**_

Ah, you raise a good point. Let me think about this more carefully:

1. The task queue user data is stored in a single Cassandra partition per namespace
2. Cassandra's consistency model means that within a single partition, all nodes should eventually have the same data
3. The error we're seeing is about not being able to poll a workflow task, not about inconsistent task queue user data

I might have been too quick to blame Cassandra's consistency model. The error suggests that no workflow task was created at all, which could mean:

1. The task queue user data was written successfully but the workflow task wasn't created
2. The task queue user data wasn't written at all
3. There's an issue with the task queue partition routing

Let me look at the test again to see what might be preventing the workflow task from being created. Would you like me to investigate these other possibilities?

---

_**User**_

Is it likely that my CI system is testing against cassandra with multiple nodes

---

_**Assistant**_

Ah, that's a crucial insight! No, it's very unlikely that your CI system is testing against a multi-node Cassandra cluster. Most CI systems run tests against a single-node Cassandra instance for simplicity and performance.

This means my earlier assumption about Cassandra consistency issues was completely wrong - with a single node, there's no eventual consistency to worry about since there's only one node to read from and write to.

We should look elsewhere for the cause of the flakiness. Would you like me to investigate other potential causes?

---

_**User**_

yes

---

_**Assistant**_

Let me look at the test failure again and the surrounding code more carefully. The error is about not being able to poll a workflow task, which suggests the task wasn't created or wasn't routed correctly.

Let me check the test setup and task queue configuration:



---

Read file: tests/versioning_3_test.go

---

Let me also look at the test that's failing:




---

Read file: tests/versioning_3_test.go

---

Looking at the code, I notice a few potential issues that could cause the flakiness:

1. **Task Queue Partitioning**: The test is using 4 partitions for both read and write (`MatchingNumTaskqueueReadPartitions` and `MatchingNumTaskqueueWritePartitions`). The error occurs in the `NoTaskForwardNoPollForwardAllowSync` configuration, which suggests there might be an issue with how tasks are being routed between partitions.

2. **New Matcher**: The test is using the new matcher (`MatchingUseNewMatcher.Key(): true`). This could be relevant if there are any differences in how the new matcher handles task routing or versioning.

3. **Ramping Configuration**: The test sets up a 50% ramp between two versions. The error suggests that tasks aren't being created, which could happen if there's an issue with how the ramping configuration is being applied.

4. **Task Queue User Data**: While we ruled out Cassandra consistency issues, there could still be an issue with how the task queue user data is being written or read, especially with the partitioned task queues.

Let me check the matching service code to see how it handles task routing and versioning. Would you like me to investigate any of these areas in particular?

---

_**User**_

could you look at a totally different test flake for me?

---

_**Assistant**_

Of course! I'd be happy to help investigate a different test flake. Could you share:

1. The test file and test name that's flaking
2. The error message or behavior you're seeing
3. Any patterns you've noticed about when it fails (e.g., specific conditions, frequency, etc.)

This will help me focus on the right area and provide more targeted assistance.

---

_**User**_

        	Error Trace:	/home/runner/work/temporal/temporal/tests/versioning_3_test.go:348
        	            				/home/runner/work/temporal/temporal/tests/versioning_3_test.go:333
        	            				/home/runner/work/temporal/temporal/tests/versioning_3_test.go:287
        	            				/home/runner/work/temporal/temporal/tests/testcore/functional_test_base.go:577
        	            				/home/runner/go/pkg/mod/github.com/stretchr/testify@v1.10.0/suite/suite.go:115
        	Error:      	Received unexpected error:
        	            	context deadline exceeded
        	Test:       	TestVersioning3FunctionalSuite/TestUnpinnedQuery_Sticky/NoTaskForwardNoPollForwardForceAsync

---

_**Assistant**_

Let me look at the test that's failing. The error is a context deadline exceeded in `TestUnpinnedQuery_Sticky` with the `NoTaskForwardNoPollForwardForceAsync` configuration.




---

Read file: tests/versioning_3_test.go

---

The error is occurring in `pollAndQueryWorkflow` which is trying to:
1. Start a query poller
2. Send a query to the workflow
3. Wait for the query result

The context deadline exceeded suggests that either:
1. The query poller isn't receiving the query task
2. The query task isn't being created
3. The query task is being created but not being routed correctly

Let me check the query workflow implementation:




---

Read file: tests/versioning_3_test.go

---

The issue appears to be in the interaction between:
1. The query poller (`pollWftAndHandleQueries`)
2. The query request (`queryWorkflow`)
3. The task queue versioning state

The test is using `NoTaskForwardNoPollForwardForceAsync` configuration, which means:
- Tasks won't be forwarded between partitions
- Polls won't be forwarded
- The query will be handled asynchronously

The context deadline exceeded suggests that the query task isn't being created or routed correctly within the 15-second timeout.

Potential causes:
1. The query task might be getting stuck in a partition that the poller isn't checking
2. The versioning state might not be properly set up when the query is sent
3. The async query handling might be failing silently

Would you like me to investigate any of these areas in particular? I can also look at the matching service code to see how it handles query tasks in this configuration.

---



---

